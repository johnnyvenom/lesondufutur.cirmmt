---

title: "Keynote 5:  Louis-Xavier Buffoni"
summary: "Thursday | 14:15 - 15:00 | Tanna Hall (McGill)"
weight: 340
tags: ["Thursday", "Keynote"]

---

<br>

| | |
| - | - |
| **Day:** | Thursday |
| **Time:** | 14:15 - 15:00 |
| **Location:** | McGill University |
| **Room:** | Tanna Schulich Hall |

## Speaker 

- [Louis-Xavier Buffoni]({{< relref "/authors/louis-xavier-buffoni" >}})

## Title

- "The Future of Interactive Audio Production"


## Description

Producing sound for interactive media, such as video games, is quite different from producing sound for films and other traditional linear media. The timing of events is fixed and known with the latter, whereas it is mostly unpredictable with the former. Interactive audio artists must create content in pieces, out of context, and this content is played back and put together in real-time by the game engine. Although tools have come a long way since artists would hand over wave file names to programmers in a huge Excel sheet to hook up in the game later on, they are still very imperfect.


Tools of the future should allow artists to produce auditory experiences that achieve the same level of quality and polish as what is done in traditional media, regardless of how events actually unfold in the real-time simulation. We propose to support this end goal with the three following pillars:

1. Nothing is pre-rendered, everything is interactive
2. Simulation is rendered to audio accurately by default
3. All creative decisions can take effect in real-time

In this presentation, I will review a few areas of research in interactive audio, and will discuss their challenges in the context of the above pillars. Doing so, I hope I can also give you a quick glimpse of the world of game audio.